{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering algorithms are a part of unsupervised machine learning algorithms. Why unsupervised ? Because, the target variable is not present. The model is trained based on given input variables which attempt to discover intrinsic groups (or clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Soft Clustering**: In this technique, the probability or likelihood of an observation being partitioned into a cluster is calculated.\n",
    "* **Hard Clustering**: In hard clustering, an observation is partitioned into exactly one cluster (no probability is calculated). <br><br>\n",
    "There are many types of clustering algorithms, such as K means, hierarchical clustering, etc. Other than these, several other methods have emerged which are used only for specific data sets or types (categorical, binary, numeric).\n",
    "> We will look into two types of algorithms and their variants\n",
    "> 1. K means\n",
    "> 2. Hierarachical\n",
    "> 3. DB Scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K Means** and **Hierarchical clustering** techniques are driven by the distance between various points and thus are usually referred as *Distance based techniques*. Hence it is necessary to look into various ways of calculating distcance *(Euclidean distance is not the only way calculate distance!)*<br><br>\n",
    ">There are some important things you should keep in mind:<br>\n",
    "> 1. With quantitative variables, distance calculations are **highly influenced by variable units and magnitude**. For example, clustering variable height (in feet) with salary (in rupees) having different units and distribution (skewed) will invariably return biased results. Hence, always make sure to standardize (mean = 0, sd = 1) the variables. **Standardization results in unit-less variables.**<br><br>\n",
    "> 2. Use of a particular **distance measure depends on the variable types**; i.e., formula for calculating distance between numerical variables is different than categorical variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.**Euclidean Distance**: It is used to calculate the distance between quantitative (numeric) variables. As it involves square terms, it is also known as L2 distance (because it squares the difference in coordinates). Its formula is given by:<br>\n",
    "\n",
    "> \\begin{align}\n",
    "\\sqrt{\\sum_{i=1}^n (x_i-y_i)^2} \n",
    "\\end{align}\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import *\n",
    " \n",
    "def euclidean_distance(x,y):\n",
    "    '''\n",
    "    Computes Euclidean Distance two 1-D vector x and y\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (N,) array_like\n",
    "        Input array\n",
    "    y : (N,) array_like\n",
    "        Input array\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Euclidean : double\n",
    "        The Euclidean distance between vectors `x` and `y`.\n",
    "    '''\n",
    "    \n",
    "    # Uncomment the line below and write your code based on the formula\n",
    "    #return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.19615242271\n"
     ]
    }
   ],
   "source": [
    "print euclidean_distance([1,2,3], [4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.**Manhattan Distance**: It is calculated as the absolute value of the sum of differences in the given coordinates. This is known as L1 distance. It is also sometimes called the Minowski Distance.<br>\n",
    "\n",
    "   An interesting fact about this distance is that it only calculates the horizontal and vertical distances. It doesn't calculate the diagonal distance. For example, in chess, we use the Manhattan distance to calculate the distance covered by rooks. Its formula is given by:<br>\n",
    "> \\begin{align}\n",
    "\\sum_{i=1}^n |x_i-y_i|\n",
    "\\end{align}\n",
    "   <br>\n",
    "Note: The generalisation of Euclidean, Manhattan distance etc. is called **Minkowski's distance**\n",
    ">\\begin{align}\n",
    "\\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p}\n",
    "\\end{align}\n",
    "when p = 2 -> Euclidean <br>\n",
    "when p = 1 -> Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import *\n",
    " \n",
    "def manhattan_distance(x,y):\n",
    "    '''\n",
    "    Computes Manhattan Distance two 1-D vector x and y\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (N,) array_like\n",
    "        Input array\n",
    "    y : (N,) array_like\n",
    "        Input array\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    manhattan : double\n",
    "        The Cosine distance between vectors `x` and `y`.\n",
    "    '''\n",
    "    # Uncomment the line below and write your code based on the formula\n",
    "    #return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print manhattan_distance([1,2,3], [4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import *\n",
    "from decimal import Decimal\n",
    " \n",
    "def nth_root(value, n_root):\n",
    "    '''\n",
    "    Computes the nth root for the given value and n\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    value : float\n",
    "            any real number\n",
    "    n : float\n",
    "        root required\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nth root : float\n",
    "                nth root of the value\n",
    "    '''\n",
    "    root_value = 1/float(n_root)\n",
    "    return round (Decimal(value) ** Decimal(root_value),3)\n",
    " \n",
    "def minkowski_distance(x,y,p_value):\n",
    "    '''\n",
    "    Computes Minkowski Distance two 1-D vector x and y for given p\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (N,) array_like\n",
    "        Input array\n",
    "    y : (N,) array_like\n",
    "        Input array\n",
    "    p : float\n",
    "        the norm factor\n",
    "        p == 1, Manhattan distance\n",
    "        p == 2, Euclidean distance\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    minkowski : double\n",
    "        The minkowski distance between vectors `x` and `y`.\n",
    "    '''\n",
    "    # Uncomment the line below and write your code based on the formula, use the nth_root function if required\n",
    "    #return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.327\n"
     ]
    }
   ],
   "source": [
    "print minkowski_distance([1,2,3], [4,5,6], p_value = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.**Hamming Distance**: It is used to calculate the distance between categorical variables. It uses a contingency table to count the number of mismatches among the observations. If a categorical variable is binary (say, male or female), it encodes the variable as male = 0, female = 1.\n",
    "\n",
    "   In case a categorical variable has more than two levels, the Hamming distance is calculated based on dummy encoding. Its formula is given by (x,y are given points):\n",
    ">\\begin{align}\n",
    "\\frac{\\sum_{i=1}^n (x_i <> y_i)}{n}\n",
    "\\end{align}\n",
    "Note: Dividing by n normalizes the distance, hamming distance is genuine without diving it by n too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hamming_distance(x, y):\n",
    "    '''\n",
    "    Computes Hamming Distance two 1-D vector x and y\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (N,) array_like\n",
    "        Input array\n",
    "    y : (N,) array_like\n",
    "        Input array\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    hamming : double\n",
    "        The hamming distance between vectors `x` and `y`.\n",
    "    '''\n",
    "    assert len(x) == len(y)\n",
    "    # Uncomment the line below and write your code based on the formula\n",
    "    #return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.666666666667\n"
     ]
    }
   ],
   "source": [
    "print hamming_distance([1,2,3], [4,2,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.**Cosine Similarity**: It is the most commonly used similarity metric in text analysis. The closeness of text data is measured by the smallest angle between two vectors. The angle (Θ) is assumed to be between 0 and 90. A quick refresher: cos (Θ = 0) = 1 and cos (Θ = 90) = 0.\n",
    "\n",
    "   Therefore, the maximum dissimilarity between two vectors is measured at Cos 90 (perpendicular). And, two vectors are said to be most similar at Cos 0 (parallel). For two vectors (x,y), the cosine similarity is given by their normalized dot product shown below:\n",
    ">\\begin{align}\n",
    "\\frac {\\pmb x \\cdot \\pmb y}{\\sqrt{(\\pmb x \\cdot \\pmb x) (\\pmb y \\cdot \\pmb y)}}\n",
    "\\end{align}\n",
    "Note: **Cosine distance = 1 - Cosine similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import *\n",
    " \n",
    "def square_rooted(x):\n",
    "    '''\n",
    "    Computes the square root of the dot product of a vector\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (N,) array_like\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    root : float\n",
    "           The square root of the dot product of a vector\n",
    "    '''\n",
    "    return round(sqrt(sum([a*a for a in x])),3)\n",
    " \n",
    "def cosine_similarity(x,y):\n",
    "    '''\n",
    "    Computes cosine similarity two 1-D vector x and y for given p\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (N,) array_like\n",
    "        Input array\n",
    "    y : (N,) array_like\n",
    "        Input array\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    cosine : double\n",
    "        The cosine similarity between vectors `x` and `y`.\n",
    "    '''\n",
    "    # Uncomment the lines below and write your code based on the formula, use the square_rooted function if required\n",
    "    \n",
    "    #numerator = \n",
    "    #denominator = \n",
    "    #return round(numerator/float(denominator),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([1,2,3], [4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.**Jaccard Coefficient**: The Jaccard coefficient (sometimes called the Jaccard similarity index) compares members for two sets to see which members are shared and which are distinct. It’s a measure of similarity for the two sets of data, with a range from 0 to 1. The higher the percentage, the more similar the two populations.<br>\n",
    "   Although it’s easy to interpret, it is extremely sensitive to small samples sizes and may give erroneous results, especially with very small samples or data sets with missing observations.\n",
    ">\\begin{align}\n",
    "\\frac {|set(x) \\cap set(y)|}{|set(x) \\cup set(y)|}\n",
    "\\end{align}\n",
    "Note: **Jaccard distance = 1 - Jaccard similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import *\n",
    " \n",
    "def jaccard_similarity(x,y):\n",
    "    '''\n",
    "    Computes jaccard similarity two 1-D vector x and y for given p\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (N,) array_like\n",
    "        Input array\n",
    "    y : (N,) array_like\n",
    "        Input array\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    jaccard : double\n",
    "        The jaccard similarity between vectors `x` and `y`.\n",
    "    '''\n",
    "    # Uncomment the line below and write your code based on the formula\n",
    "    #intersection_cardinality = \n",
    "    #union_cardinality = \n",
    "    #return intersection_cardinality/float(union_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "print jaccard_similarity([1,2,3], [1,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-built library function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For faster calculation on huge datasets, use in-built functions as they are optimized.<br>\n",
    "We will be using \n",
    "\n",
    "1. distance from scipy.spatial library, for more info:<br>\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html#scipy.spatial.distance.cdist\n",
    "    <br>\n",
    "    <br>\n",
    "2. DistanceMetric from sklearn.neighbors library, for more info:<br>\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.19615242]]\n",
      "[[ 9.]]\n",
      "[[ 4.32674871]]\n",
      "[[ 0.66666667]]\n",
      "[[ 0.02536815]]\n",
      "[[ 0.66666667]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "euclidean_dist = distance.cdist(XA = [[1,2,3]], XB = [[4,5,6]], metric = \"euclidean\") \n",
    "print euclidean_dist\n",
    "\n",
    "manhattan_dist = distance.cdist(XA = [[1,2,3]], XB = [[4,5,6]], metric = \"cityblock\") \n",
    "print manhattan_dist\n",
    "\n",
    "minkowski_dist = distance.cdist(XA = [[1,2,3]], XB = [[4,5,6]], metric = \"minkowski\", p = 3) \n",
    "print minkowski_dist\n",
    "\n",
    "hamming_dist = distance.cdist(XA = [[1,2,3]], XB = [[4,2,6]], metric = \"hamming\") \n",
    "print hamming_dist\n",
    "\n",
    "cosine_dist = distance.cdist(XA = [[1,2,3]], XB = [[4,5,6]], metric = \"cosine\") \n",
    "print cosine_dist\n",
    "\n",
    "jaccard_dist = distance.cdist(XA = [[1,2,3]], XB = [[1,5,6]], metric = \"jaccard\") \n",
    "print jaccard_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.19615242]]\n",
      "[[ 9.]]\n",
      "[[ 4.32674871]]\n",
      "[[ 0.66666667]]\n",
      "[[ 0.]]\n"
     ]
    }
   ],
   "source": [
    "#importing the required library\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "\n",
    "euclidean_dist = DistanceMetric.get_metric('euclidean')\n",
    "print euclidean_dist.pairwise(X = [[1,2,3]], Y = [[4,5,6]])\n",
    "\n",
    "manhattan_dist = DistanceMetric.get_metric('manhattan')\n",
    "print manhattan_dist.pairwise(X = [[1,2,3]], Y = [[4,5,6]])\n",
    "\n",
    "minkowski_dist = DistanceMetric.get_metric('minkowski', p = 3)\n",
    "print minkowski_dist.pairwise(X = [[1,2,3]], Y = [[4,5,6]])\n",
    "\n",
    "hamming_dist = DistanceMetric.get_metric('hamming')\n",
    "print hamming_dist.pairwise(X = [[1,2,4]], Y = [[4,2,6]])\n",
    "\n",
    "jaccard_dist = DistanceMetric.get_metric('jaccard')\n",
    "print jaccard_dist.pairwise(X = [[1,2,3]], Y = [[1,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iot_product",
   "language": "python",
   "name": "iot_product"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
