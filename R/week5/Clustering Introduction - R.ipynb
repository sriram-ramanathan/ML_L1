{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering algorithms are a part of unsupervised machine learning algorithms. Why unsupervised ? Because, the target variable is not present. The model is trained based on given input variables which attempt to discover intrinsic groups (or clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Soft Clustering**: In this technique, the probability or likelihood of an observation being partitioned into a cluster is calculated.\n",
    "* **Hard Clustering**: In hard clustering, an observation is partitioned into exactly one cluster (no probability is calculated). <br><br>\n",
    "There are many types of clustering algorithms, such as K means, hierarchical clustering, etc. Other than these, several other methods have emerged which are used only for specific data sets or types (categorical, binary, numeric).\n",
    "> We will look into two types of algorithms and their variants\n",
    "> 1. K means\n",
    "> 2. Hierarachical\n",
    "> 3. DB Scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K Means** and **Hierarchical clustering** techniques are driven by the distance between various points and thus are usually referred as *Distance based techniques*. Hence it is necessary to look into various ways of calculating distcance *(Euclidean distance is not the only way calculate distance!)*<br><br>\n",
    ">There are some important things you should keep in mind:<br>\n",
    "> 1. With quantitative variables, distance calculations are **highly influenced by variable units and magnitude**. For example, clustering variable height (in feet) with salary (in rupees) having different units and distribution (skewed) will invariably return biased results. Hence, always make sure to standardize (mean = 0, sd = 1) the variables. **Standardization results in unit-less variables.**<br><br>\n",
    "> 2. Use of a particular **distance measure depends on the variable types**; i.e., formula for calculating distance between numerical variables is different than categorical variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.**Euclidean Distance**: It is used to calculate the distance between quantitative (numeric) variables. As it involves square terms, it is also known as L2 distance (because it squares the difference in coordinates). Its formula is given by:<br>\n",
    "\n",
    "> \\begin{align}\n",
    "\\sqrt{\\sum_{i=1}^n (x_i-y_i)^2} \n",
    "\\end{align}\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "euclideanDistance <- function(x,y){\n",
    "    \n",
    "    #     Computes Euclidean Distance two 1-D vector x and y\n",
    "\n",
    "    #     Args:\n",
    "    #     x : (N,) vector\n",
    "    #         Input vector\n",
    "    #     y : (N,) vector\n",
    "    #         Input vector\n",
    "\n",
    "    #     Returns:\n",
    "    #     Euclidean : double\n",
    "    #         The Euclidean distance between vectors `x` and `y`.\n",
    "    \n",
    "    # Uncomment the line below and write your code based on the formula\n",
    "    \n",
    "    #return()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "5.19615242270663"
      ],
      "text/latex": [
       "5.19615242270663"
      ],
      "text/markdown": [
       "5.19615242270663"
      ],
      "text/plain": [
       "[1] 5.196152"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "euclideanDistance(c(1,2,3), c(4,5,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.**Manhattan Distance**: It is calculated as the absolute value of the sum of differences in the given coordinates. This is known as L1 distance. It is also sometimes called the Minowski Distance.<br>\n",
    "\n",
    "   An interesting fact about this distance is that it only calculates the horizontal and vertical distances. It doesn't calculate the diagonal distance. For example, in chess, we use the Manhattan distance to calculate the distance covered by rooks. Its formula is given by:<br>\n",
    "> \\begin{align}\n",
    "\\sum_{i=1}^n |x_i-y_i|\n",
    "\\end{align}\n",
    "   <br>\n",
    "Note: The generalisation of Euclidean, Manhattan distance etc. is called **Minkowski's distance**\n",
    ">\\begin{align}\n",
    "\\left(\\sum_{i=1}^n |x_i-y_i|^p\\right)^{1/p}\n",
    "\\end{align}\n",
    "when p = 2 -> Euclidean <br>\n",
    "when p = 1 -> Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manhattanDistance <- function(x,y){\n",
    "    \n",
    "#     Computes Manhattan Distance two 1-D vector x and y\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     x : (N,) vector\n",
    "#         Input vector\n",
    "#     y : (N,) vector\n",
    "#         Input vector\n",
    "        \n",
    "#     Returns\n",
    "#     -------\n",
    "#     manhattan : double\n",
    "#         The Cosine distance between vectors `x` and `y`.\n",
    "    \n",
    "    # Uncomment the line below and write your code based on the formula\n",
    "    \n",
    "    #return()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "9"
      ],
      "text/latex": [
       "9"
      ],
      "text/markdown": [
       "9"
      ],
      "text/plain": [
       "[1] 9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "manhattanDistance(c(1,2,3), c(4,5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minkowskiDistance <- function(x,y,p_value){\n",
    "    \n",
    "#     Computes Minkowski Distance two 1-D vector x and y for given p\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     x : (N,) vector\n",
    "#         Input vector\n",
    "#     y : (N,) vector\n",
    "#         Input vector\n",
    "#     p : float\n",
    "#         the norm factor\n",
    "#         p == 1, Manhattan distance\n",
    "#         p == 2, Euclidean distance\n",
    "        \n",
    "#     Returns\n",
    "#     -------\n",
    "#     minkowski : double\n",
    "#         The minkowski distance between vectors `x` and `y`.\n",
    "    \n",
    "    # Uncomment the line below and write your code based on the formula\n",
    "    \n",
    "    #return()\n",
    "    }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "4.32674871092222"
      ],
      "text/latex": [
       "4.32674871092222"
      ],
      "text/markdown": [
       "4.32674871092222"
      ],
      "text/plain": [
       "[1] 4.326749"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minkowskiDistance(c(1,2,3), c(4,5,6), p_value = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.**Hamming Distance**: It is used to calculate the distance between categorical variables. It uses a contingency table to count the number of mismatches among the observations. If a categorical variable is binary (say, male or female), it encodes the variable as male = 0, female = 1.\n",
    "\n",
    "   In case a categorical variable has more than two levels, the Hamming distance is calculated based on dummy encoding. Its formula is given by (x,y are given points):\n",
    ">\\begin{align}\n",
    "\\frac{\\sum_{i=1}^n (x_i <> y_i)}{n}\n",
    "\\end{align}\n",
    "Note: Dividing by n normalizes the distance, hamming distance is genuine without diving it by n too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hammingDistance <- function(x, y){\n",
    "#     Computes Hamming Distance two 1-D vector x and y\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     x : (N,) vector\n",
    "#         Input vector\n",
    "#     y : (N,) vector\n",
    "#         Input vector\n",
    "        \n",
    "#     Returns\n",
    "#     -------\n",
    "#     hamming : double\n",
    "#         The hamming distance between vectors `x` and `y`.\n",
    "    if(length(x) == length(y)){\n",
    "        # Uncomment the line below and write your code based on the formula\n",
    "        #return()\n",
    "    }\n",
    "    return(F)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.666666666666667"
      ],
      "text/latex": [
       "0.666666666666667"
      ],
      "text/markdown": [
       "0.666666666666667"
      ],
      "text/plain": [
       "[1] 0.6666667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hammingDistance(c(1,2,3), c(4,2,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.**Cosine Similarity**: It is the most commonly used similarity metric in text analysis. The closeness of text data is measured by the smallest angle between two vectors. The angle (Θ) is assumed to be between 0 and 90. A quick refresher: cos (Θ = 0) = 1 and cos (Θ = 90) = 0.\n",
    "\n",
    "   Therefore, the maximum dissimilarity between two vectors is measured at Cos 90 (perpendicular). And, two vectors are said to be most similar at Cos 0 (parallel). For two vectors (x,y), the cosine similarity is given by their normalized dot product shown below:\n",
    ">\\begin{align}\n",
    "\\frac {\\pmb x \\cdot \\pmb y}{\\sqrt{(\\pmb x \\cdot \\pmb x) (\\pmb y \\cdot \\pmb y)}}\n",
    "\\end{align}\n",
    "Note: **Cosine distance = 1 - Cosine similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosineSimilarity <- function(x,y){\n",
    "#     Computes cosine similarity two 1-D vector x and y for given p\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     x : (N,) vector\n",
    "#         Input vector\n",
    "#     y : (N,) vector\n",
    "#         Input vector\n",
    "        \n",
    "#     Returns\n",
    "#     -------\n",
    "#     cosine : double\n",
    "#         The cosine similarity between vectors `x` and `y`.\n",
    "    \n",
    "    # Uncomment the line below and write your code based on the formula\n",
    "    \n",
    "    #numerator = \n",
    "    #denominator = \n",
    "    #return(round(numerator/denominator,3))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.975"
      ],
      "text/latex": [
       "0.975"
      ],
      "text/markdown": [
       "0.975"
      ],
      "text/plain": [
       "[1] 0.975"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cosineSimilarity(c(1,2,3), c(4,5,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.**Jaccard Coefficient**: The Jaccard coefficient (sometimes called the Jaccard similarity index) compares members for two sets to see which members are shared and which are distinct. It’s a measure of similarity for the two sets of data, with a range from 0 to 1. The higher the percentage, the more similar the two populations.<br>\n",
    "   Although it’s easy to interpret, it is extremely sensitive to small samples sizes and may give erroneous results, especially with very small samples or data sets with missing observations.\n",
    ">\\begin{align}\n",
    "\\frac {|set(x) \\cap set(y)|}{|set(x) \\cup set(y)|}\n",
    "\\end{align}\n",
    "Note: **Jaccard distance = 1 - Jaccard similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jaccardSimilarity <- function(x,y){\n",
    "#     Computes jaccard similarity two 1-D vector x and y for given p\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     x : (N,) vector\n",
    "#         Input vector\n",
    "#     y : (N,) vector\n",
    "#         Input vector\n",
    "        \n",
    "#     Returns\n",
    "#     -------\n",
    "#     jaccard : double\n",
    "#         The jaccard similarity between vectors `x` and `y`.\n",
    "    \n",
    "    # Uncomment the line below and write your code based on the formula\n",
    "    \n",
    "    #intersection_cardinality = \n",
    "    #union_cardinality = \n",
    "    #return(intersection_cardinality/union_cardinality)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.2"
      ],
      "text/latex": [
       "0.2"
      ],
      "text/markdown": [
       "0.2"
      ],
      "text/plain": [
       "[1] 0.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jaccardSimilarity(c(1,2,3), c(1,5,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-built library function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For faster calculation on huge datasets, use in-built functions as they are optimized.<br>\n",
    "We will be using \n",
    "\n",
    "1. Distance from 'philentropy' package. It supports around 40 distance/Similarity measures<br>\n",
    "    **Note: It gives similarity score for similarity measures, subtract the value from 1 to get the distance**<br>\n",
    "    For more information see:<br>\n",
    "    https://cran.r-project.org/web/packages/philentropy/vignettes/Distances.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "euclidean \n",
      " 5.196152 \n",
      "manhattan \n",
      "        9 \n",
      "minkowski \n",
      " 4.326749 \n",
      "    cosine \n",
      "0.02536815 \n",
      "  jaccard \n",
      "0.6170213 \n"
     ]
    }
   ],
   "source": [
    "if (!is.element('philentropy', installed.packages()[,1])){\n",
    "    install.packages('philentropy', dep = TRUE)\n",
    "}\n",
    "library('philentropy')\n",
    "\n",
    "euclideanDist = distance(rbind(c(1,2,3), c(4,5,6)), method = \"euclidean\")\n",
    "print(euclideanDist)\n",
    "\n",
    "manhattanDist = distance(rbind(c(1,2,3), c(4,5,6)), method = \"manhattan\")\n",
    "print(manhattanDist)\n",
    "\n",
    "minkowskiDist = distance(rbind(c(1,2,3), c(4,5,6)), method = \"minkowski\",p = 3 )\n",
    "print(minkowskiDist)\n",
    "\n",
    "cosineSim = distance(rbind(c(1,2,3), c(4,5,6)), method = \"cosine\")\n",
    "print(1 - cosineSim)\n",
    "\n",
    "jaccardSim = distance(rbind(c(1,2,3), c(1,5,6)), method = \"jaccard\")\n",
    "print(1 - jaccardSim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
